\documentclass[ngerman,pdftex,paper=A4,DIV=calc,titlepage,12pt]{scrartcl}

%Global options
\usepackage[paper=a4paper,includefoot,includehead,left=20mm,right=20mm,top=20mm,bottom=20mm]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage[ngerman]{babel}
%\usepackage[%colorlinks=true,
	    %linkcolor=red,
	    %anchorcolor=black,
	    %citecolor=green,
	    %pagecolor=red,
	    %urlcolor=cyan
	    %]
	    %{hyperref}
\usepackage{nameref}

\usepackage{hyperref}

%Page header
\usepackage{fancyhdr}
\usepackage{fancyhdr} 
\pagestyle{fancyplain}
\headheight\baselineskip
\topmargin-0.75cm
\textheight47\baselineskip
\def\MakeUppercase#1{#1}
\makeatletter
\lhead[\fancyplain{}{\thepage}]
      {\fancyplain{}{\slshape Burrows-Wheeler-Transformation}} % <--- Titel eintragen
\rhead[\fancyplain{}{\slshape Uli Köhler	}]    % <--- Name eintragen
      {\fancyplain{}{\thepage}}
\cfoot[]{}
\makeatother


%Theorems
\usepackage{thmbox} %Boxed theorems

%Tables, Floats and figures
\usepackage{array}
\usepackage{float} %COnfigure figure floats to be boxed
  \floatstyle{boxed}
  \restylefloat{figure}

%Special formats
\usepackage{url}

%Citations and references
\usepackage{cite}
\usepackage[german]{fancyref}
\usepackage[german]{varioref}
\renewcommand{\reftextfaceafter}{auf der \reftextvario{gegenüberliegenden}{nächsten} Seite} 
\renewcommand{\reftextfacebefore}{auf der \reftextvario{gegenüberliegenden}{vorherigen} Seite} 
\renewcommand{\reftextafter}{auf der \reftextvario{nächsten}{folgenden} Seite} 
\renewcommand{\reftextbefore}{auf \reftextvario{der vorhergehenden}{der letzten} Seite} 
\renewcommand{\reftextcurrent}{auf \reftextvario{der aktuellen}{dieser} Seite}


%Typography, language and error corrections:
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} %Latin modern = enhanced CM font
\usepackage{xspace} %Space enhancements
\usepackage[tracking=true,activate={true,nocompatibility},babel=true]{microtype} %PDFTeX typography enhancements
\usepackage{fixltx2e}
%Line spacing
\usepackage{setspace}

\usepackage{inconsolata}


%Header declarations
\pagestyle{headings}

%Create a new boxed type of theorems
\newtheorem[L]{boxedDefinition}{Definition}
\newtheorem{definition}{Definition}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

%Scriptsized, vertically and horizontally centered tabular column type
\newcolumntype{s}[1]{>{\scriptsize\centering\arraybackslash}m{#1}}

\title{3D-Tumorvisualisation}
\subtitle{Seminararbeit}
\author{Uli Köhler}
%\institute[EMG]{Ernst-Mach-Gymnasium Haar}
\date{9.~November 2010}

\newcommand{\footnoteremember}[2]{\footnote{#2}\newcounter{#1}\setcounter{#1}{\value{footnote}}}
\newcommand{\footnoterecall}[1]{\footnotemark[\value{#1}]}
%Utility to insert a newline after a paragraph declaration
\newcommand{\paranl}{$~~$\\}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\sloppy
\begin{document}
\begin{titlepage}
\begin{center}
 Proseminar \glqq Algorithmen der Bioinformatik\grqq
 \end{center}
\vspace{2cm}
\begin{center}
 \large\textsc{Ausarbeitung}\\[5mm]
 {\Huge\centering\bfseries\selectfont Textkompression:\\Burrows-Wheeler-Transformation}\\[2cm]
\begin{center}
  Uli Köhler\\
  12.11.2012
\end{center}
\vspace{2cm}
\end{center}
\tableofcontents
\end{titlepage}
\section{Einleitung}
Im Vorliegenden Dokument soll die Bedeutung der Burrows-Wheeler-Transformation (im Folgenden auch BWT genannt) für die Textkompression und deren Anwendung in der Bioinformatik behandelt werden.

Moderne Sequenzierverfahren erzeugen durch Vielfachabdeckung von Genomen große Datenmengen. Um die Rohdaten speichern zu können, müssen spezielle Kompressionsalgorithmen angewandt werden, um den benötigten Speicherplatz auf ein Minimum zu reduzieren.
\section{Verlustfreie Kompression}
\subsection{ Kompression} \label{ssec:bigdata-bioinformatics}
In vielen realen Datensätzen sind starke Redundanzen enthalten. Oft ist die Größe des Alphabets nicht sehr groß (z.B. 36 Alphanumerische Zeichen plus Sonderzeichen in englischen Texten oder 4 Nukleinbasen in DNA- oder RNA-Sequenzen) oder Texte sind stark autokorreliert.

Kompressionsalgorithmen sind Verfahren, die diese Redundanzen eliminieren und dadurch eine platzsparende Speicherung der Daten erlauben, wärend die ursprünglichen unkomprimierten Daten jederzeit vollständig rekonstruiert werden können.
Dadurch unterscheiden sich verlustfreie Algorithmen von verlustbehafteten Verfahren, die zwar eine oft deutlich größere Platzeinsparung erzeugen, aber dafür keine vollständige Rekonstruktion erlauben.
Verschiedene Kompressionsalgorithmen sind für verschiedene Typen von Datensätzen und Datenbanken geeignet - insbesondere können schnellere Algorithmen meist besser komprimieren als langsamere Algorithmen. Für jeden Anwendungsfall muss so zwischen den Faktoren Kompressionszeit, Dekompressionszeit und Speicherplatzeinsparung abgewogen werden.
\section{Die Burrows-Wheeler-Transformation}
Die Burrows-Wheeler-Transformation ist selbst kein Kompressionsalgorithmus, sondern ein Verfahren, um einen Text reversibel so zu verändern, dass er sich durch einige\footnote{Durch die Resultate der im Rahmen dieser Arbeit entwickelten Software \texttt{eBWT} kann gezeigt werden, dass insbesondere mit Dictionary-basierten Kompressionsalgorithmen der von der BWT erzeugt Text oft schlechter kompressibel ist als der Originaltext} Kompressionsalgorithmen deutlich besser komprimieren lässt als der Originaltext.
\subsection{Die Burrows-Wheeler-Hintransformation}\label{ssec:transformation}
Die Hintransformation wird detailiert in \cite{Heun2003} sowie in \cite[Seite 2 - Algorithmus C]{burrows1994block} beschrieben.
Gegeben sei der zu transformierende Text \textit{S}. Zuerst wird eine Matrix $M$ der Größe $|S| \times |S|$ initialisiert.

$M$ wird nun mit allen zyklischen Rotationen von \textit{S} gefüllt - die Reihenfolge (und damit die Richtung der Rotation) spielt hierbei keine Rolle.

Nun werden die Zeilen von $M$ lexikographisch sortiert. In der sortierten Matrix $M_S$ wird nun der Originaltext \textit{S} gesucht und dessen Index als \textit{I} gespeichert. Sei $L$ nun der Spaltenvektor der letzten Spalte von $M_S$. Dann ist das Resultat der Hintransformation das Tupel $(L, I)$.
\subsection{Die Burrows-Wheeler-Rücktransformation}
\label{ssec:backtransformation}
Die Rücktransformation wird detailiert in \cite[Seite 3-5, Algorithmus D]{burrows1994block} sowie \cite{Heun2003} beschrieben.
Ausgehend von $M_S$ wird die Matrix $M_S'$ als Rotation von $M_S$ um ein Zeichen nach rechts definiert. Da $M_S$ ausschließlich Rotationen von $S$ enthält, enthält auch $M_S'$ Rotationen. Der Vektor $T$ der Dimension $|S|$ wird nun als Korrelation von $M_S$ zu $M_S'$ wie Folgt berechnet:\\
\textit{Für jede Zeile $Z_i$ von $M_S'$ suche in $M_S$ die Zeile $Z_j$, die zu $Z_i$ äquivalent ist und setze $T_i = j$}
Durch die Konstruktion ist bekannt, dass für alle $0 \leq j \leq |S|-1$ gilt: $L_j$ ist $S$ der direkte zyklische Nachfolger von $L_{T_j}$
Sei nun $T^i$ wie Folgt definiert: ${T^0}_x = x\ ;\ {T^{i+1}}_x = T_{{T^i}_x}$
Dann kann $S$ wie Folgt rekonstrukiert werden: \textit{Für alle $0 \leq i \leq |S|$ gilt: $S_{N-1-i} = L_{{T^i}_I}$}
\subsection{Vereinfachtes Verfahren zur Rücktransformation}
Das Folgende Verfahren ist eine vereinfachte Version der Rücktransformation, die weniger komplex ist als die in Kapitel \vref{ssec:backtransformation} vorgestellte, dafür allerdings mehr Rechenzeit benötigt. Da sie einfacher zu erklären ist, wird sie im Vortrag ausführlicher als die ursprüngliche Rücktransformation dargestellt. Detailiert wird diese Variante der Rücktransformation in \cite{Wik2012-1} dargestellt.

Gegeben sei das Tupel $(L,I)$ - das Resultat der Hintransformation.
Durch das Kompressionsverfahren ist bekannt, dass der letzte Spaltenvektor von $M$ $L$ ist. Zudem wurde M zuvor lexikographisch sortiert, sodass zudem bekannt ist, dass der erste Spaltenvektor von $M$ - der durch die Rotation dieselben Zeichen mit derselben Häufigkeit wie $L$ enthält - dadurch kann der erste Spaltenvektor von $M$ ausgefüllt werden, indem die Zeichen in $L$ lexikographisch sortiert werden.

Wir definieren nun die Matrix $M'$, indem wir $M$ um ein Zeichen nach links rotieren. Dadurch ist in $M'$ lediglich die letzte und die vorletzte Spalte bekannt.\
\noindent Im weiteren Verlauf werden die Folgenden Schritte auf $M'$ wiederholt, bis $M'$ voll ist:
\begin{enumerate}
 \item Füllen der freien Spalte mit dem größten Index mit $L$
 \item Lexikographisches Sortieren der gesamten Matrix $M'$
\end{enumerate}
Wenn $M'$ voll ist, ist $M'$ vollständig äquivalent zu $M$.
\section{Komprimierbarkeit der transformierten Textes}
Die Gründe, warum der tranformierte Text in vielen Fällen besser komprimierbar ist als der Originaltext, werden in \cite{burrows1994block} detailiert dargestellt. Weitere Beispiele finden sich auch in \cite{Wik2012-1}.

Betrachten wir Beispielsweise einen englischen Text und die Matrix $M'$ aus der Hintransformation (Siehe auch Kapitel \vref{ssec:transformation}).

Statistisch lässt sich feststellen, dass in englischen Texten einige Kombinationen aus aufeinanderfolgenden Buchstaben häufiger vorkommen als andere, beispielsweise kommt \textit{th} in \textit{the} deutlich öfter vor als \textit{qy}.

Sofern wir nur diejenigen Zeilen in $M'$ betrachten, die mit dem zweiten Buchstaben eines häufig vorkommenden Buchstabenpaars beginnen, so wird durch die Rotation und lexikographische Sortierung sichergestellt, dass    in vielen aufeinanderfolgenden Zeilen der erste Buchstabe des Buchstabenpaars in der letzten Zeile steht. Da nun $L$ durch den letzten Spaltenvektor definiert ist, werden durch solche Buchstabenpaare lange Runs in $L$ erzeugt. Von entsprechenden Kompressionsalgorithmen können diese Runs ausgenutzt werden.

\subsection{Blockbasierte Kompression}
Gerade bei großen Datensätzen bedeutet die Sortierung einer $n \times n$-Matrix einen enormen Zeitaufwand, der in vielen Anwendungsfällen nicht akzeptabel ist. Um diesem Problem entgegenzuwirken, wird der Originaltext meist in mehrere Blöcke gleicher Größe aufgeteilt - dadurch wird zwar oft nicht die maximale mögliche Kompression erreicht, da Korrelationen zwischen den Blöcken bei der BWT nicht berücksichtigt werden, der Zeitaufwand aber sinkt bei kleinerer Blockgröße enorm.

\subsection{Kombination mit anderen Algorithmen}
Die im Rahmen dieser Arbeit entwickelte Software \textit{eBWT}\footnote{extreme BWT} führt auf einen Bioinformatik-Datensatz\footnote{Chromosom III von \textit{C. Elegans} als FASTA-Datei, Quelle: \url{ftp://hgdownload.cse.ucsc.edu/goldenPath/ce6/chromosomes/}} und den \textit{Fictional Text} aus dem Calgary-Kompressionstestkorpus verschiedene Kombinationen von Algorithmen aus und prüft für verschiedene Blockgrößen die Größe der resultierenden Dateien.

eBWT implementiert einen Speichereffizienten Rotierungs- und Sortierungsalgorithmus in C++11, der auf einem modifizierten \textit{Heapsort}-Algorithmus aufbaut. Dieser speichert statt den zu sortierenden Strings nur einen Zeiger und erzielt dadurch eine Speicherkomplexität von $\mathcal{O}(n)$ bezüglich der Blockgröße.

Auf die Repräsentation des Indizes I aus dem Resultat der Hintransformation sowie des Dictionaries des Move-To-Front-Encodings wird verzichtet, da die Wahl der Repräsentation große Einflüsse auf die resultierende Dateigröße haben könnte.

Untersucht wurde insbesondere die Frage, inwiefern die Kombination der BWT mit den in \cite{burrows1994block} vorgeschlagenen Algorithmen \textit{Move-To-Front-Encoding} (MTF) und \textit{Huffman-Kodierung} zur Erreichung einer hohen Speicherplatzeinsparung notwendig ist.

Zum Zeitpunkt der Erstellung dieser Ausarbeitung konnte aufgrund hoher Entwicklungs- und Rechenzeiten nur eine kleine Zahl der möglichen Experimente mit eBWT durchgeführt werden. Die Folgenden Resultate wurden bisher erzielt:
\begin{itemize}
 \item Mit der BWT transformierte Texte lassen sich mit Dictionary-basierten Algorithmen wie \textit{Deflate} meist schlechter komprimieren als die entsprechenden Originaltexte
 \item Übereinstimmend mit den Resultaten in \cite{burrows1994block} wurde festgestellt, dass unter jeder möglichen Kombination von BWT, MTF und Huffman-Kodierung unabhängig von der Blöckgröße die Kombination BWT \textrightarrow MTF \textrightarrow Huffman-Codierung die kleinsten Dateien erzeugt
 \item Die Kombination von MTF und Huffman-Kodierung (ohne BWT) erzeugt beim Bioinformatik-Testdatensatz eine deutlich bessere Kompression als beim englischen Text
 \item Eine Vergrößerung der Blockgröße über etwa 1e+5 hinaus hat nur einen sehr kleinen Effekt, beim Bioinformatik-Datensatz ist dieser Effekt allerdings größer als beim englischen Text
 \item Beim Bioinformatik-Datensatz ist der Effekt einer Vergrößerung der Blockgröße auf die resultierende Dateigröße im Allgemeinen deutlich kleiner als beim englischen Text.
\end{itemize}
Insgesamt legen diese Resultate nahe, dass signifikante Unterschiede zwischen dem englischen Text und dem Bioinformatik-Datensatz bestehen. Die Resultate aus nicht-bioinformatikbezogenen Publikationen müssen daher im Bezug auf Bioinformatik-Datesätze mit Vorsicht betrachtet werden.
\subsection{Komplexität und Laufzeitverhalten}\label{ssec:complexity}
Die Burrows-Wheeler-Transformation hat die asymptotische Komplexität $\mathcal{O}(n^2)$. Die Sortierung während der Transformation nimmt von den Teilschritten des Verfahrens am meisten Rechenzeit in Anspruch - daher hängt das Laufzeitverhalten insbesondere von der Wahl der Sortierungsalgorithmus ab.

Burrows und Wheeler stellen in \cite{burrows1994block} einen modifizierten Quicksort-Algorithmus vor, der für den von ihnen verwendeten Calgary-Kompressionstestkorpus das beste Laufzeitverhalten zeigte. Das lässt sich allerdings nicht notwendigerweise auf andere Datensätze - insbesondere nicht auf Bioinformatik-Datensätze - verallgemeinern.
Die meisten modernen Sortieralgorithmen unterscheiden sich in der Average-Case- und der Worst-Case-Komplexität. Allerdings sind auf Bäumen basierende Algorithmen in vielen Fällen auf konkreter Hardware langsamer als in der Theorie.

Der Grund dafür liegt vor allem darin, dass der Speicher, in dem der Baum und seine Attribute gespeichert sind, aufgrund von Änderungen, z.B. an der Baumstruktur häufig neu alloziert werden muss. Dadurch wird in vielen Fällen die Repräsentation des Baums im Arbeitsspeicher fragmentiert.

CPUs können aber nicht direkt auf den Arbeitsspeicher zugreifen, sondern müssen kleinere Einheiten - sogenannte \textit{Pages}\footnote{Meist 4096 Bytes groß auf Linux x86\_64-Systemen} - zuerst in einen CPU-nahen mehrstufigen, aber insgesamt nur wenige Megabyte großen Cache laden. Sofern auf eine Page zugegriffen werden soll, die sich nicht im Cache befindet, wird ein sogenannter \textit{page fault} ausgelöst und die Page aus dem Arbeitsspeicher in den Cache geladen. Dieser Prozess dauert allerdings im Vergleich zu Rechenoperationen sehr lange und verzögert daher den Programmablauf.

Aufgrund der Fragmentierung der Repräsentation von Bäumen im Speicher ist die Wahrscheinlichkeit groß, dass bei einem gegebenen Algorithmus zwei Knoten oder Kanten des Baums, auf die nacheinander zugegriffen wird, in zwei verschiedenen Pages liegen. Unter Umständen befindet sich diese Page aber nicht mehr im Cache und muss daher aus dem Arbeitsspeicher geladen werden. 

Aufgrund dieses Effektes sind Baumbasierte Sortieralgorithmen auch für die Burrows-Wheeler-Transformation nicht immer die beste Wahl, selbst wenn ihre theoretische Komplexität für eine definierte Menge an Datensätzen ein besseres Laufzeitverhalten nahelegt. In der Praxis müssen oft verschiedene Algorithmen getestet werden, um den zu finden, der für die zu transformierenden Daten das beste Laufzeitverhalten zeigt.

Einen detailierteren Einblick in diese Thematik bietet \cite{Drepper2007}.
\subsection{Resultate von Cox und Bauer}
In \cite{cox2012large} wird eine Anwendung einer modifizierten Version der BWT (\textit{\glqq BWT-SAP\grqq}) auf eine Datenbank von DNA-Sequenzen beschrieben.

Insbesondere wird die BWT-SAP benutzt, um die durch die durch die starke Korrelation sich überlappender Sequenzen entstehende Redundanz zu verringern (Siehe auch \vref{ssec:bigdata-bioinformatics}).

Cox und Bauer benutzen dafür eine auf \textit{Suffix Arrays} basierende Methode, um große Datenmengen effizient komprimieren zu können. Dadurch treffen die unter \vref{ssec:complexity} beschriebenen Nachteile von Baumbasierten Algorithmen nciht zu.
\section{Zusammenfassung}
Im Kontext einer allgemeinen Einführung über die Prinzipien verlustfreier Kompression wurde auf die Burrows-Wheeler-Transformation - ein Algorithmus, um Texte besser komprimierbar zu machen - eingegangen.

\noindent Verschiedene Methoden für Hin- und die Rücktransformation wurden vorgestellt. 

Anhand der für diesen Zweck entwickelten Software \textit{eBWT} wurde analysiert, welche Algorithmen sich besonders für die Anwendung nach der BWT eignen und wie stark Unterschiede zwischen englischen Texten und Bioinformatikdatensätzen ausgeprägt ist.

Zudem wurde die Wahl des Sortierungsalgorithmus diskutiert, wobei besonders auf die Nachteile von Baumbasierten Algorithmen auf konkreter Hardware eingegangen wurde.
\renewcommand\refname{Literatur- und Quellenverzeichnis}
\bibliographystyle{alphadin}
\bibliography{library}
\subsection*{Open Data}\label{opendata}
Alle für die Erstellung dieser Ausarbeitung sowie für die Präsentation verwendeten Quelldateien und Rohdaten können dauerhaft unter \url{https://github.com/ulikoehler/Proseminar} abgerufen werden
\end{document}
